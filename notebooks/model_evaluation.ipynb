{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tmb2k01/deep_learning/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(os.path.join(project_root, \"src\"))\n",
    "\n",
    "from model.ensemble import EnsembleModel\n",
    "from model.common import CommonPLModuleWrapper\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from monai.losses import DiceLoss\n",
    "from monai.networks.nets import UNETR, SegResNet\n",
    "from monai.networks.utils import one_hot\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from dataloader import BrainTumourDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = \"../data/BrainTumourData/imagesTr/\"\n",
    "LABEL_PATH = \"../data/BrainTumourData/labelsTr/\"\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 1\n",
    "IN_CHANNELS = 4\n",
    "OUT_CHANNELS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = BrainTumourDataModule(\n",
    "    data_path=IMAGE_PATH,\n",
    "    seg_path=LABEL_PATH,\n",
    "    img_dim=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "data_module.prepare_data()\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "segresnet = CommonPLModuleWrapper(\n",
    "    model=SegResNet(in_channels=IN_CHANNELS, out_channels=OUT_CHANNELS),\n",
    "    loss_fn=DiceLoss(softmax=True),\n",
    ")\n",
    "segres_weights = torch.load(\n",
    "    f\"../model/{segresnet.model.__class__.__name__}.ckpt\", weights_only=True\n",
    ")\n",
    "segresnet.load_state_dict(segres_weights[\"state_dict\"])\n",
    "\n",
    "unet = CommonPLModuleWrapper(\n",
    "    model=UNETR(\n",
    "        in_channels=IN_CHANNELS,\n",
    "        out_channels=OUT_CHANNELS,\n",
    "        img_size=(IMG_SIZE, IMG_SIZE, IMG_SIZE),\n",
    "    ),\n",
    "    loss_fn=DiceLoss(softmax=True),\n",
    ")\n",
    "unet_weights = torch.load(\n",
    "    f\"../model/{unet.model.__class__.__name__}.ckpt\", weights_only=True\n",
    ")\n",
    "unet.load_state_dict(unet_weights[\"state_dict\"])\n",
    "\n",
    "model = EnsembleModel([segresnet, unet], num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (128) must match the existing size (4) at non-singleton dimension 1.  Target sizes: [4, 128, 128, 128].  Tensor sizes: [4, 128, 128]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     17\u001b[0m         predictions \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m---> 18\u001b[0m         \u001b[43mall_predictions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcurrent_index\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(predictions[\u001b[38;5;241m0\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     19\u001b[0m         all_targets[current_index] \u001b[38;5;241m=\u001b[39m targets[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     21\u001b[0m all_predictions \u001b[38;5;241m=\u001b[39m one_hot(all_predictions\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), num_classes)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (128) must match the existing size (4) at non-singleton dimension 1.  Target sizes: [4, 128, 128, 128].  Tensor sizes: [4, 128, 128]"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "total_batches = len(data_module.test_dataloader())\n",
    "num_classes = 4\n",
    "height, width = 128, 128\n",
    "slices_per_sample = 128\n",
    "\n",
    "all_predictions = torch.empty(\n",
    "    (total_batches, num_classes, slices_per_sample, height, width), dtype=torch.float32\n",
    ")\n",
    "all_targets = torch.empty(\n",
    "    (total_batches, num_classes, slices_per_sample, height, width), dtype=torch.float32\n",
    ")\n",
    "\n",
    "for current_index, (images, targets) in enumerate(data_module.test_dataloader()):\n",
    "    with torch.no_grad():\n",
    "        predictions = model(images)\n",
    "        all_predictions[current_index] = torch.argmax(predictions[0], dim=0)\n",
    "        all_targets[current_index] = targets[0]\n",
    "\n",
    "all_predictions = one_hot(all_predictions.unsqueeze(1), num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.metrics import (\n",
    "    DiceMetric,\n",
    "    compute_confusion_matrix_metric,\n",
    "    get_confusion_matrix,\n",
    "    compute_dice,\n",
    ")\n",
    "\n",
    "dice_metric = compute_dice(\n",
    "    y_pred=all_predictions, y=all_targets, ignore_empty=False, num_classes=4\n",
    ")\n",
    "mean_dice = dice_metric.mean(dim=0)\n",
    "print(f\"Dice Coefficient: {mean_dice}\")\n",
    "print(f\"Mean Dice Coefficient: {mean_dice.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_name = {\n",
    "    0: \"Background\",\n",
    "    1: \"Edema\",\n",
    "    2: \"Non-Enhancing Tumor\",\n",
    "    3: \"Enhancing Tumor\",\n",
    "}\n",
    "\n",
    "predicted_classes = torch.argmax(all_predictions, dim=1).flatten().cpu().numpy()\n",
    "target_classes = torch.argmax(all_targets, dim=1).flatten().cpu().numpy()\n",
    "conf_matrix = confusion_matrix(target_classes, predicted_classes)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    conf_matrix, columns=index_to_name.values(), index=index_to_name.values()\n",
    ")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = get_confusion_matrix(\n",
    "    y_pred=all_predictions, y=all_targets, include_background=False\n",
    ")\n",
    "cm = confusion_matrix.sum(dim=0)\n",
    "print(f\"Confusion Matrix: \\n{cm}\")\n",
    "\n",
    "recall = compute_confusion_matrix_metric(\n",
    "    confusion_matrix=cm,\n",
    "    metric_name=\"sensitivity\",\n",
    ")\n",
    "print(f\"Recall: {recall}\")\n",
    "\n",
    "alpha = torch.tensor([0.2, 0.3, 0.5])\n",
    "weighted_recall = (recall * alpha).sum()\n",
    "print(f\"Weighted Recall: {weighted_recall}\")\n",
    "\n",
    "precision = compute_confusion_matrix_metric(\n",
    "    confusion_matrix=cm,\n",
    "    metric_name=\"precision\",\n",
    ")\n",
    "print(f\"Precision: {precision}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

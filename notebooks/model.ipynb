{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(os.path.join(project_root, \"src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from matplotlib.axes import Axes\n",
    "from monai.losses import DiceLoss\n",
    "from monai.networks.nets import SegResNet, UNETR\n",
    "from torch import Tensor\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataloader import BrainTumourDataModule\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping SegResNet\n",
    "\n",
    "The input data must have a shape of (B, N, H, W, D) format\n",
    "\n",
    "- B - batch size\n",
    "- N - number of classes\n",
    "- H - height\n",
    "- W - width\n",
    "- D - depth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegResModel(pl.LightningModule):\n",
    "    def __init__(self, in_channels, out_channels, learning_rate=1e-3):\n",
    "        super(SegResModel, self).__init__()\n",
    "        self.model = SegResNet(in_channels=in_channels, out_channels=out_channels)\n",
    "        self.dice_loss = DiceLoss(softmax=True)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch: Tuple[Tensor, Tensor], batch_idx: int) -> Tensor:\n",
    "        loss, _ = self._common_step(batch, batch_idx)\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, _ = self._common_step(batch, batch_idx)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def _common_step(\n",
    "        self, batch: Tuple[Tensor, Tensor], batch_idx: int\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "        images, labels = batch\n",
    "        preds = self.forward(images)\n",
    "        loss = self.dice_loss(preds, labels)\n",
    "        return loss, preds\n",
    "\n",
    "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetModel(pl.LightningModule):\n",
    "    def __init__(self, in_channels, out_channels, learning_rate=1e-3):\n",
    "        super(UNetModel, self).__init__()\n",
    "        self.model = UNETR(\n",
    "            in_channels=4,\n",
    "            out_channels=4,\n",
    "            img_size=(128, 128, 128),\n",
    "        )\n",
    "        self.dice_loss = DiceLoss(softmax=True)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch: Tuple[Tensor, Tensor], batch_idx: int) -> Tensor:\n",
    "        loss, _ = self._common_step(batch, batch_idx)\n",
    "        return loss\n",
    "\n",
    "    def _common_step(\n",
    "        self, batch: Tuple[Tensor, Tensor], batch_idx: int\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "        images, labels = batch\n",
    "        preds = self.forward(images)\n",
    "        loss = self.dice_loss(preds, labels)\n",
    "        return loss, preds\n",
    "\n",
    "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel(pl.LightningModule):\n",
    "    def __init__(self, model_list, num_classes):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.models: List[pl.LightningModule] = model_list\n",
    "        self.num_classes: int = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "\n",
    "        predictions = [model(x) for model in self.models]\n",
    "        averaged_prediction = torch.mean(torch.stack(predictions), dim=0)\n",
    "        return averaged_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Module Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"../data/BrainTumourData/imagesTr/\"\n",
    "label_path = \"../data/BrainTumourData/labelsTr/\"\n",
    "img_dim = (128, 128)\n",
    "batch_size = 1\n",
    "\n",
    "data_module = BrainTumourDataModule(\n",
    "    data_path=image_path, seg_path=label_path, img_dim=img_dim, batch_size=batch_size\n",
    ")\n",
    "data_module.prepare_data()\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate each model\n",
    "segresnet = SegResModel(in_channels=4, out_channels=4)\n",
    "\n",
    "# Train each model separately\n",
    "wandb_logger = pl.loggers.WandbLogger(\n",
    "    project=\"medical-image-segmentation\", log_model=\"all\"\n",
    ")\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(monitor=\"val_loss\", mode=\"min\")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")\n",
    "trainer.fit(segresnet, data_module)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate each model\n",
    "unet = UNetModel(in_channels=4, out_channels=4)\n",
    "\n",
    "# Train each model separately\n",
    "wandb_logger = pl.loggers.WandbLogger(\n",
    "    project=\"medical-image-segmentation\", log_model=\"all\"\n",
    ")\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(monitor=\"val_loss\", mode=\"min\")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")\n",
    "trainer.fit(unet, data_module)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = EnsembleModel([segresnet, unet], num_classes=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list: List[Tensor] = []\n",
    "prediction_list: List[Tensor] = []\n",
    "label_list: List[Tensor] = []\n",
    "\n",
    "for images, label in tqdm(data_module.test_dataloader()):\n",
    "    with torch.no_grad():\n",
    "        pred: Tensor = ensemble_model(images)\n",
    "        image_list.append(images)\n",
    "        prediction_list.append(pred)\n",
    "        label_list.append(label)\n",
    "\n",
    "images: Tensor = torch.cat(image_list, dim=0)\n",
    "predictions: Tensor = torch.cat(prediction_list, dim=0)\n",
    "labels: Tensor = torch.cat(label_list, dim=0)\n",
    "\n",
    "print(f\"images shape: {images.shape}\")\n",
    "print(f\"predictions shape: {predictions.shape}\")\n",
    "print(f\"labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_slices(image: Tensor, label: Tensor, pred: Tensor, slice_index: int) -> None:\n",
    "    image_slice: np.ndarray = image[0, :, :, slice_index].cpu().numpy()\n",
    "    labels_map: Tensor = torch.argmax(label, dim=0)\n",
    "    preds_map: Tensor = torch.argmax(pred, dim=0)\n",
    "\n",
    "    labels_slice: np.ndarray = labels_map[:, :, slice_index].cpu().numpy()\n",
    "    preds_slice: np.ndarray = preds_map[:, :, slice_index].cpu().numpy()\n",
    "\n",
    "    axes: Tuple[Axes, Axes, Axes]\n",
    "    _, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    axes_orig: Axes = axes[0]\n",
    "    axes_truth: Axes = axes[1]\n",
    "    axes_pred: Axes = axes[2]\n",
    "\n",
    "    axes_orig.imshow(image_slice, cmap=\"gray\")\n",
    "    axes_orig.set_title(\"Original MRI Image\")\n",
    "    axes_orig.axis(\"off\")\n",
    "\n",
    "    axes_truth.imshow(labels_slice, cmap=\"jet\")\n",
    "    axes_truth.set_title(\"Ground Truth\")\n",
    "    axes_truth.axis(\"off\")\n",
    "\n",
    "    axes_pred.imshow(preds_slice, cmap=\"jet\")\n",
    "    axes_pred.set_title(\"Predictions\")\n",
    "    axes_pred.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "image = images[idx]\n",
    "print(f\"image shape: {image.shape}\")\n",
    "\n",
    "label = labels[idx]\n",
    "print(f\"labels shape: {label.shape}\")\n",
    "\n",
    "pred = predictions[idx]\n",
    "print(f\"preds shape: {pred.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slices(image, label, pred, slice_index=image.shape[3] // 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
